{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install -q -U tensorflow-text\n",
        "!pip install -q -U tf-models-official\n",
        "!pip install -U tfds-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79yQzenragku",
        "outputId": "271f74be-32a4-4883-f37e-3b80b41ee0c5"
      },
      "id": "79yQzenragku",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 58.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 47.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 71.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.1 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.4.0.dev202201040107-py3-none-any.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.62.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.17.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tfds-nightly) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n",
            "Installing collected packages: tfds-nightly\n",
            "Successfully installed tfds-nightly-4.4.0.dev202201040107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xin-util -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_9BNadU4PQR",
        "outputId": "a8fbb65d-9f31-4a05-b903-1c866e839dff"
      },
      "id": "y_9BNadU4PQR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xin-util\n",
            "  Downloading xin_util-1.4.13-py3-none-any.whl (398 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 398 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: xin-util\n",
            "Successfully installed xin-util-1.4.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aphsv-q4dI5v",
        "outputId": "d6939fe1-57c2-46ac-ba8c-d105169f938a"
      },
      "id": "aphsv-q4dI5v",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f671240b-942c-47cb-82a7-1dca51b40601",
      "metadata": {
        "id": "f671240b-942c-47cb-82a7-1dca51b40601",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# !pip install tensorflow-addons\n",
        "# !pip install -q -U tensorflow-text\n",
        "# !pip install -q -U tf-models-official\n",
        "# !pip install -U tfds-nightly\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# import os\n",
        "# import tensorflow as tf\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "# import tensorflow_addons as tfa # For metrics\n",
        "# from official.nlp import optimization\n",
        "\n",
        "# tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# \"\"\"\n",
        "# Configure TFHub to read checkpoints directly from TFHub's Cloud Storage buckets. This is only recommended when running TFHub models on TPU.\n",
        "# Without this setting TFHub would download the compressed file and extract the checkpoint locally. Attempting to load from these local files will fail with the following error:\n",
        "# ```\n",
        "# InvalidArgumentError: Unimplemented: File system scheme '[local]' not implemented\n",
        "# ```\n",
        "# This is because the [TPU can only read directly from Cloud Storage buckets](https://cloud.google.com/tpu/docs/troubleshooting#cannot_use_local_filesystem).\n",
        "# Note: This setting is automatic in Colab.\n",
        "# \"\"\"\n",
        "# os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\n",
        "\n",
        "\n",
        "# class BERT_FineTune:\n",
        "#     def __init__(self, handle_encoder, handle_preprocess):\n",
        "#       try:\n",
        "#         resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "#         tf.config.experimental_connect_to_cluster(resolver)\n",
        "#         tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#         strategy = tf.distribute.TPUStrategy(resolver)\n",
        "#         print('Using TPU')\n",
        "#       except:\n",
        "#         if tf.config.list_physical_devices('GPU'):\n",
        "#             strategy = tf.distribute.MirroredStrategy()\n",
        "#             print('Using GPU')\n",
        "#         else:\n",
        "#             strategy = None\n",
        "#             print('Running on CPU is not recommended.')\n",
        "#       self.strategy = strategy\n",
        "#       self.handle_encoder = handle_encoder\n",
        "#       self.handle_preprocess = handle_preprocess\n",
        "\n",
        "#     @staticmethod\n",
        "#     def make_bert_preprocess_model(handle_preprocess, sentence_features, seq_length=128):\n",
        "#         \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "#         Args:\n",
        "#         sentence_features: a list with the names of string-valued features.\n",
        "#         seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "#         Returns:\n",
        "#         A Keras Model that can be called on a list or dict of string Tensors\n",
        "#         (with the order or names, resp., given by sentence_features) and\n",
        "#         returns a dict of tensors for input to BERT.\n",
        "#         \"\"\"\n",
        "\n",
        "#         input_segments = [\n",
        "#             tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "#             for ft in sentence_features]\n",
        "\n",
        "#         # Tokenize the text to word pieces.\n",
        "#         bert_preprocess = hub.load(handle_preprocess)\n",
        "#         tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "#         segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "#         # Optional: Trim segments in a smart way to fit seq_length.\n",
        "#         # Simple cases (like this example) can skip this step and let\n",
        "#         # the next step apply a default truncation to approximately equal lengths.\n",
        "#         truncated_segments = segments\n",
        "\n",
        "#         # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "#         # are model-dependent, so this gets loaded from the SavedModel.\n",
        "#         packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "#                                 arguments=dict(seq_length=seq_length),\n",
        "#                                 name='packer')\n",
        "#         model_inputs = packer(truncated_segments)\n",
        "#         return tf.keras.Model(input_segments, model_inputs)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def convert_dataset(df, batch_size, bert_preprocess_model, sentence_features, label='label', shuffle=False, repeat=False):\n",
        "#         AUTOTUNE = tf.data.AUTOTUNE\n",
        "#         in_memory_ds = dict()\n",
        "#         for feature in sentence_features:\n",
        "#             in_memory_ds[feature] = df[feature]\n",
        "#         in_memory_ds['label'] = df[label]\n",
        "\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds)\n",
        "#         num_examples = len(in_memory_ds['label'])\n",
        "\n",
        "#         if shuffle:\n",
        "#             dataset = dataset.shuffle(num_examples)\n",
        "#         if repeat:\n",
        "#             dataset = dataset.repeat()\n",
        "#         dataset = dataset.batch(batch_size)\n",
        "#         dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
        "#         dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "#         return dataset, num_examples\n",
        "\n",
        "#     @staticmethod\n",
        "#     def build_classifier_model(encoder, num_classes, seed=0):\n",
        "#         tf.random.set_seed(seed)\n",
        "\n",
        "#         class Classifier(tf.keras.Model):\n",
        "#             def __init__(self, encoder, num_classes):\n",
        "#                 super(Classifier, self).__init__(name=\"prediction\")\n",
        "#                 self.encoder = encoder\n",
        "#                 self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "#                 self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "#             def call(self, preprocessed_text):\n",
        "#                 encoder_outputs = self.encoder(preprocessed_text)\n",
        "#                 pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "#                 x = self.dropout(pooled_output)\n",
        "#                 x = self.dense(x)\n",
        "#                 return x\n",
        "\n",
        "#         model = Classifier(encoder, num_classes)\n",
        "#         return model\n",
        "\n",
        "\n",
        "#     def tune(self, train_df, val_df=None, sentence_features=['sentence'], label='label', epochs=3, batch_size=32, optimizer='sgd', seq_length=128):\n",
        "#         num_classes = len(train_df[label].unique())\n",
        "#         bert_preprocess_model = BERT_FineTune.make_bert_preprocess_model(self.handle_preprocess, sentence_features, seq_length)\n",
        "\n",
        "#         train_dataset, train_data_size = BERT_FineTune.convert_dataset(\n",
        "#             train_df, batch_size, bert_preprocess_model, sentence_features, label, True, True)\n",
        "\n",
        "#         steps_per_epoch = train_data_size // batch_size\n",
        "#         num_train_steps = steps_per_epoch * epochs\n",
        "#         num_warmup_steps = num_train_steps // 10\n",
        "        \n",
        "#         if val_df is None:\n",
        "#             validation_dataset = None\n",
        "#             validation_steps = None\n",
        "#         else:\n",
        "#             validation_dataset, validation_data_size = BERT_FineTune.convert_dataset(\n",
        "#                 val_df, batch_size, bert_preprocess_model, sentence_features, label, False, False)\n",
        "#             validation_steps = validation_data_size // batch_size\n",
        "\n",
        "        \n",
        "#         if self.strategy is None:\n",
        "#             encoder = hub.KerasLayer(self.handle_encoder, trainable=True)\n",
        "\n",
        "#             # metric have to be created inside the strategy scope\n",
        "\n",
        "#             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "#             #metrics = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
        "#             classifier_model = BERT_FineTune.build_classifier_model(encoder, num_classes)\n",
        "\n",
        "#             # optimizer = optimization.create_optimizer(\n",
        "#             #     init_lr=init_lr,\n",
        "#             #     num_train_steps=num_train_steps,\n",
        "#             #     num_warmup_steps=num_warmup_steps,\n",
        "#             #     optimizer_type='adamw')\n",
        "\n",
        "#             classifier_model.compile(optimizer=optimizer, loss=loss, metrics='accuracy') # metrics=[metrics]\n",
        "\n",
        "#             classifier_model.fit(\n",
        "#                 x=train_dataset,\n",
        "#                 validation_data=validation_dataset,\n",
        "#                 steps_per_epoch=steps_per_epoch,\n",
        "#                 epochs=epochs,\n",
        "#                 validation_steps=validation_steps)\n",
        "#         else:\n",
        "#             with self.strategy.scope():\n",
        "#                 encoder = hub.KerasLayer(self.handle_encoder, trainable=True)\n",
        "\n",
        "#                 # metric have to be created inside the strategy scope\n",
        "\n",
        "#                 loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "#                 #metrics = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
        "\n",
        "#                 classifier_model = BERT_FineTune.build_classifier_model(encoder, num_classes)\n",
        "\n",
        "#                 # optimizer = optimization.create_optimizer(\n",
        "#                 #     init_lr=init_lr,\n",
        "#                 #     num_train_steps=num_train_steps,\n",
        "#                 #     num_warmup_steps=num_warmup_steps,\n",
        "#                 #     optimizer_type='adamw')\n",
        "\n",
        "#                 classifier_model.compile(optimizer=optimizer, loss=loss, metrics='accuracy') # metrics=[metrics]\n",
        "\n",
        "#                 classifier_model.fit(\n",
        "#                     x=train_dataset,\n",
        "#                     validation_data=validation_dataset,\n",
        "#                     steps_per_epoch=steps_per_epoch,\n",
        "#                     epochs=epochs,\n",
        "#                     validation_steps=validation_steps)\n",
        "#         return classifier_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data/IMDB Dataset.csv')\n",
        "data=data.head(100)\n",
        "#data = pd.read_csv('/content/drive/MyDrive/Data/positive_data.csv')"
      ],
      "metadata": {
        "id": "PmuN7IwDb3wL"
      },
      "id": "PmuN7IwDb3wL",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {'positive': 1, 'negative': 0}\n",
        "data.sentiment.replace(label_map, inplace=True)\n"
      ],
      "metadata": {
        "id": "UguoGWnCbPlq"
      },
      "id": "UguoGWnCbPlq",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msk = np.random.rand(len(data)) < 0.8\n",
        "\n",
        "train_df = data[msk]\n",
        "\n",
        "val_df = data[~msk]"
      ],
      "metadata": {
        "id": "7q_PuItHiCZ7"
      },
      "id": "7q_PuItHiCZ7",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "jK9gGWD0L-Wa",
        "outputId": "81f2f07d-31ca-4c12-d53d-d3b8008db344"
      },
      "id": "jK9gGWD0L-Wa",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a203da20-c785-4626-bbda-6509d3428a8f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>\"The Cell\" is an exotic masterpiece, a dizzyin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>This film tried to be too many things all at o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>I've just watched Fingersmith, and I'm stunned...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>This film laboured along with some of the most...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>Maybe it was the title, or the trailer (certai...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>Oh God, I must have seen this when I was only ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Oh noes one of these attack of the Japanese gh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Hollywood movie industry is the laziest one in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>If you came here, it's because you've already ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Daniel Day-Lewis is the most versatile actor a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>This IS the worst movie I have ever seen, as w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>I have been a Mario fan for as long as I can r...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a203da20-c785-4626-bbda-6509d3428a8f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a203da20-c785-4626-bbda-6509d3428a8f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a203da20-c785-4626-bbda-6509d3428a8f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               review  sentiment\n",
              "1   A wonderful little production. <br /><br />The...          1\n",
              "12  So im not a big fan of Boll's work but then ag...          0\n",
              "15  Kind of drawn in by the erotic scenes, only to...          0\n",
              "17  This movie made it into one of my top 10 most ...          0\n",
              "26  \"The Cell\" is an exotic masterpiece, a dizzyin...          1\n",
              "27  This film tried to be too many things all at o...          0\n",
              "61  I've just watched Fingersmith, and I'm stunned...          0\n",
              "69  This film laboured along with some of the most...          0\n",
              "77  Maybe it was the title, or the trailer (certai...          0\n",
              "82  Oh God, I must have seen this when I was only ...          0\n",
              "87  Oh noes one of these attack of the Japanese gh...          0\n",
              "89  Hollywood movie industry is the laziest one in...          0\n",
              "91  If you came here, it's because you've already ...          0\n",
              "95  Daniel Day-Lewis is the most versatile actor a...          1\n",
              "98  This IS the worst movie I have ever seen, as w...          0\n",
              "99  I have been a Mario fan for as long as I can r...          1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ModelHub.BERT import BERT_FineTune"
      ],
      "metadata": {
        "id": "qCaRh9Mt4VMQ"
      },
      "id": "qCaRh9Mt4VMQ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = ['review']"
      ],
      "metadata": {
        "id": "aLdTAO32cwE9"
      },
      "id": "aLdTAO32cwE9",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "47ca33e2-750b-4548-acd5-f897315790dc",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47ca33e2-750b-4548-acd5-f897315790dc",
        "outputId": "f7338166-7bf0-4a6d-82cd-2870683c4594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CPU is not recommended.\n"
          ]
        }
      ],
      "source": [
        "# bert_finetune = BERT_FineTune(handle_encoder='/home/jupyter/BertPretained/bert_en_uncased_L-12_H-768_A-12', \n",
        "#                               handle_preprocess='/home/jupyter/BertPreprocess/bert_en_uncased_preprocess')\n",
        "\n",
        "bert_finetune = BERT_FineTune(handle_encoder='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3', \n",
        "                              handle_preprocess='https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74709d3d-a5c0-4cc9-8c71-d9c7b0929dea",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74709d3d-a5c0-4cc9-8c71-d9c7b0929dea",
        "outputId": "a0ae3342-c13a-432b-b233-e8d775099086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1250/1250 [==============================] - 2202s 2s/step - loss: 0.3608 - accuracy: 0.8356 - val_loss: 0.2752 - val_accuracy: 0.8818\n",
            "Epoch 2/2\n",
            "1250/1250 [==============================] - 2183s 2s/step - loss: 0.2664 - accuracy: 0.8881 - val_loss: 0.2787 - val_accuracy: 0.8802\n"
          ]
        }
      ],
      "source": [
        "bert_model = bert_finetune.tune(train_df, val_df, feature_cols,\n",
        "                   label='sentiment', epochs=2,\n",
        "                   batch_size=32, optimizer='sgd', seq_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mqDYNSsAorPU"
      },
      "id": "mqDYNSsAorPU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3nxf5s8VorSA"
      },
      "id": "3nxf5s8VorSA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2e85b133-d7e0-4570-aa55-3e54827c33f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e85b133-d7e0-4570-aa55-3e54827c33f6",
        "outputId": "02aeca25-25e7-401b-edb4-a673edd75900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        }
      ],
      "source": [
        "bert_preprocess_model = bert_finetune.make_bert_preprocess_model(bert_finetune.handle_preprocess,feature_cols, seq_length=128)\n",
        "test_ds, _=bert_finetune.convert_dataset(val_df, 32, \n",
        "                          bert_preprocess_model, \n",
        "                          feature_cols, \n",
        "                          label='sentiment', shuffle=False, repeat=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f60ceaf-57de-4754-a91f-6b740ee079a3",
      "metadata": {
        "id": "4f60ceaf-57de-4754-a91f-6b740ee079a3"
      },
      "outputs": [],
      "source": [
        "with bert_finetune.strategy.scope():\n",
        "    predictions = bert_model.predict(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "102e6fc1-e2ce-4399-a09e-00d0ed2a4901",
      "metadata": {
        "id": "102e6fc1-e2ce-4399-a09e-00d0ed2a4901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2381c285-3e84-4b79-ed51-102b99ea594f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8798348030624741"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "pred = np.argmax(predictions, axis=1)\n",
        "real = val_df['sentiment']\n",
        "f1_score(real, pred, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ed3a840a-3978-4f6a-815c-43b1bcf1398e",
      "metadata": {
        "id": "ed3a840a-3978-4f6a-815c-43b1bcf1398e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(\n",
        "    bert_model, '/content/drive/MyDrive/Data/bert', overwrite=True, include_optimizer=True, save_format=None,\n",
        "    signatures=None, options=None, save_traces=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "CH5W9ZcGFMUZ",
        "outputId": "0aea57b3-4072-4966-f0ef-7f5967f55ec5"
      },
      "id": "CH5W9ZcGFMUZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e0a34b86160b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tf.keras.models.save_model(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Data/bert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msignatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_traces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bert_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.save('/content/drive/MyDrive/Data/bert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlTYi4FZclBp",
        "outputId": "2a3e638e-a5e6-4bf8-a21f-bfcf88287bf0"
      },
      "id": "vlTYi4FZclBp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 910). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load method 1"
      ],
      "metadata": {
        "id": "F6mRDjWIeh-k"
      },
      "id": "F6mRDjWIeh-k"
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_loaded = tf.keras.models.load_model('/content/drive/MyDrive/Data/bert')"
      ],
      "metadata": {
        "id": "JmRFmI0fGKqd"
      },
      "id": "JmRFmI0fGKqd",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_loaded.predict(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GelGpW4weYSp",
        "outputId": "86a6900e-2c1f-467b-8f89-497f44c2efb1"
      },
      "id": "GelGpW4weYSp",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.2573388 ,  2.1636329 ],\n",
              "       [ 0.41836604, -0.2902224 ],\n",
              "       [ 2.560231  , -3.053667  ],\n",
              "       [ 2.8376184 , -2.3377025 ],\n",
              "       [-1.938671  ,  1.6335995 ],\n",
              "       [ 2.6738029 , -2.133901  ],\n",
              "       [ 2.5080783 , -2.1009414 ],\n",
              "       [ 2.6132226 , -2.2868807 ],\n",
              "       [ 0.86485064, -0.79451114],\n",
              "       [ 2.2084835 , -1.91831   ],\n",
              "       [ 2.350346  , -2.1415944 ],\n",
              "       [ 1.6419716 , -1.4620467 ],\n",
              "       [ 1.5054134 , -1.2570333 ],\n",
              "       [-1.4324028 ,  1.5337418 ],\n",
              "       [ 3.1358187 , -2.735223  ],\n",
              "       [-1.0985657 ,  1.2649761 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(bert_model_loaded.predict(test_ds), axis=1)"
      ],
      "metadata": {
        "id": "jtbbAtDNepcz"
      },
      "id": "jtbbAtDNepcz",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real = val_df['sentiment']"
      ],
      "metadata": {
        "id": "jfiV7vY9e6xq"
      },
      "id": "jfiV7vY9e6xq",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "f1_score(real, pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQCdUDtBfHSa",
        "outputId": "c238bea1-1550-46d9-967e-727b9ed74e27"
      },
      "id": "SQCdUDtBfHSa",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load method 2"
      ],
      "metadata": {
        "id": "cQRhq5kheqG4"
      },
      "id": "cQRhq5kheqG4"
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_loaded = tf.saved_model.load(\n",
        "    '/content/drive/MyDrive/Data/bert', tags=None, options=None\n",
        ")\n"
      ],
      "metadata": {
        "id": "TV3jfPBCer6H"
      },
      "id": "TV3jfPBCer6H",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = bert_model_loaded.signatures[\"serving_default\"]"
      ],
      "metadata": {
        "id": "tDndDJJObcMA"
      },
      "id": "tDndDJJObcMA",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_point = list(test_ds)[0][0]"
      ],
      "metadata": {
        "id": "ZsvIzy3Pfc0g"
      },
      "id": "ZsvIzy3Pfc0g",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(f(input_mask = test_point['input_mask'], input_type_ids = test_point['input_type_ids'], input_word_ids = test_point['input_word_ids'])['output_1'].numpy(), axis=1)"
      ],
      "metadata": {
        "id": "T1-H0Py7dulv"
      },
      "id": "T1-H0Py7dulv",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(f(**test_point)['output_1'].numpy(), axis=1)"
      ],
      "metadata": {
        "id": "8qBO18FNi_9k"
      },
      "id": "8qBO18FNi_9k",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 32\n",
        "real = val_df.head(batch_size)['sentiment']\n",
        "f1_score(real, pred, average='macro')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD5dwlASessN",
        "outputId": "020920ff-e9f9-4f34-c042-f2884453675a"
      },
      "id": "xD5dwlASessN",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pbqtt1Tfg4dI"
      },
      "id": "pbqtt1Tfg4dI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-7.m86",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m86"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "BERTClass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}